---
title: "MANOVA"
author: "Shravan Kuchkula"
date: "9/20/2017"
output:
  html_document:
    keep_md: yes
    theme: cosmo
    toc: yes
  github_document:
    toc: yes
  pdf_document:
    fig_caption: yes
    highlight: zenburn
---

MANOVA is geared towards doing a simple ANOVA model with multiple response variables.

LDA on the other hand, says hey, “using the response variables, can we predict what factor level the observation is coming from?”.

## Introduction
The hemophilia data set contains two measured variables on 75 women, belonging to two groups: n1=30 of them are non-carriers (normal group) and n2=45 are known hemophilia A carriers (obligatory carriers).  MANOVA and LDA are linked together in terms of what is going on under the hood mathematically.  MANOVA though is geared towards doing a simple ANOVA model with multiple response variables.  LDA on the other hand, says hey, “using the response variables, can we predict what factor level the observation is coming from?”. The variables in the study are:


* **Group** - Categorical variable with levels: Non-carriers or Carriers
* **Activity** - Continuous variable.
* **Antigen** - Continuous variable.

## Data gathering and cleaning
All the required libraries are loaded.

```{r message = FALSE, warning = FALSE}
source('libraries.R')
```

The data is retrieved and stored into a data frame using `read.csv`.

```{r}
hemophil <- read.csv("/Users/Shravan/Downloads/Unit7Exercise/Hemophil.csv")
```


## Exploratory Data Analysis
Summary statistics per response variable by group. Use the `describeBy` from `psych` package.

```{r}
describeBy(hemophil, hemophil$Group)
```

Draw a boxplot to visualize the effect of a categorical predictor on a continuous response variable. 
First we will draw Activity ~ Group and then we will draw Antigen ~ Group.

```{r}
ggplot(data = hemophil, aes(x = Group, y = Activity)) +
  geom_boxplot() +
  ggtitle("Activity vs Group")
```

```{r}
ggplot(data = hemophil, aes(x = Group, y = Antigen)) +
  geom_boxplot() +
  ggtitle("Antigen vs Group")
```

Our main question is, Are the Activity/Antigen means for the 2 groups statistically different from one another ? Well, looking at the box plot for Activity/Antigen by group definitely seems that they are different, but we want to formally answer this question using a one-way ANOVA model.

### Understanding one-way ANOVA.
## ANOVA in R
Normally, you do not have to do all calculations yourself to get the F-value and to see whether or not the null hypothesis (i.e. that all groups are equal) should be rejected. R's aov() function does the heavy lifting for you! Apply aov() to the hemophil data. The only argument should be a formula containing the dependent variable iq and independent variable condition. For example, aov(dependent_var ~ independent_var).

```{r}
# use aov for Activity
anova_hemophil <- aov(hemophil$Activity ~ hemophil$Group)

# use aov for Antigen
anova_antigen <- aov(hemophil$Antigen ~ hemophil$Group)

# summarize both the models
summary(anova_hemophil)
summary(anova_antigen)
```

The p-value is < 0.05, which means, we reject the null hypothesis and state that atleast one of the means of the sub-groups is different. This is true for both Activity and Antigen response variables.

For Activity: The F-value is 23.82, which is really large and the p-value is really small. As a result, you have 23.82 times as much between group variance as within group variance, so you have a big effect. 
For Antigen: The F-value is 4.303, which is not that large and thus the p-value is close to 0.05. As a result, you have 4.3 times as much between group variance as within group variance, so you have a resonable effect.

These can also be visually confirmed through box plots shown above.


### Checking ANOVA assumptions
The assumptions of ANOVA are relatively simple. Similar to an independent t-test, we have a continuous dependent variable, which we assume to be normally distributed. Furthermore, we assume homogeneity of variance, which can be tested with Levene's test. It's good practice to check both assumptions before you do ANOVA, but here we'll focus on the latter. If the assumptions don't hold, then the ANOVA results won't be valid.


Perform Levene's test for the hemophil data. Use `leveneTest` from the `car` package with the dependent and independent variables as first and second arguments, respectively.

```{r}
# Perform leveneTest for both Activity and Antigen.
leveneTest(hemophil$Activity, hemophil$Group)
```
If you don't specify additional arguments, the deviation scores are calculated by comparing each score to its group median. This is the default behavior, even though they are typically calculated by comparing each score to its group mean. If you want to use means and not medians, add an argument `center = mean`. Do this now and compare the results to the first test.

```{r}
# Perform leveneTest for both Activity and Antigen.
leveneTest(hemophil$Activity, hemophil$Group, center = mean)
leveneTest(hemophil$Antigen, hemophil$Group, center = mean)
```

In both cases the p-value is not significant, which means, we will fail to reject the null hypothesis - which is that of equal variances. 

## Post Hoc tests
The F-test showed a significant effect somewhere among the groups. However, it did not tell you which pairwise comparisons are significant. This is where post-hoc tests come into play. They help you to find out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.

What does it mean to inflate the type I error?

Suppose the post-hoc test involves performing three pairwise comparisons, each with the probability of a type I error set at 5%. The probability of making at least one type I error is then equal to 

$$ 1−(no:type:I:error:×:no:type:I:error:×:no:type:I:error) $$

If, for simplicity, you assume independence of these three events, the maximum familywise error rate becomes 1−(0.95×0.95×0.95)=14.26%. In other words, the probability of having at least one false alarm (i.e. type I error) is 14.26%.

Anytime you engage in NHST*, a type I error can occur. In a situation were you do multiple pairwise comparisons, the probability of type I errors in the process inflates substantially. Therefore, it is better to build in adjustments to take this into account. This is what Tukey tests and other post-hoc procedures do. They adjust the p-value to prevent inflation of the type I error rate.

> *Null Hypothesis Significance Testing (NHST) is a statistical method used to test whether or not you are able to reject or retain the null hypothesis. This type of test can confront you with a type I error. This happens when the test rejects the null hypothesis, while it is actually true in reality. Furthermore, the test can also deliver a type II error. This is the failure to reject a null hypothesis when it is false. All hypothesis tests have a probability of making type I and II errors.

> TYPE I Error: When comparing the means for the levels of a factor in an analysis of variance, a simple comparison using t-tests will inflate the probability of declaring a significant difference when it is not in fact present. "Detecting a significant difference when it is not in fact present! FALSE POSITIVE."

### Tukey's procedure

In R you can use the Tukey's procedure via `TukeyHSD()` function.
```{r}
# Conduct ANOVA
anova_activity <- aov(hemophil$Activity ~ hemophil$Group)

# summarize the model
summary(anova_activity)

# Conduct Tukey procedure
(tukey <- TukeyHSD(anova_activity, ordered=TRUE))

# Plot it
plot(tukey)

```

### Bonferroni adjusted p-values
Just like Tukey's procedure, the Bonferroni correction is a method that is used to counteract the problem of inflated type I errors while engaging in multiple pairwise comparisons between subgroups. Bonferroni is generally known as the most conservative method to control the familywise error rate.


## Understanding Covariance

Provide a scatterplot for Carrier vs Non-Carrier for your final variables used in #2 and color code the points by group. Since we know each variable satistifies normality from #2, the only additional question is if the data are following multivariate normality with a constant covariance matrix for each group. Try to explain what visual properties you see in the plot would lead you for or against the assumption of constant covariance matrix. Recall covariance matrix is simply book keeping variance and correlation estimates for the two variables.  

```{r}
ggplot(data = hemophil, aes(x = Activity, y = Antigen, color = Group)) +
  geom_point()
```

There are two ways of expressing the relationship between variables:

* **Covariance** 
* **Correlation**

Covariance refers to how much two variables are associated (i.e., whether two variables covary). To understand covariance, you’ll need to understand the variance and standard deviation of a single variable. Variance or standard deviation represents the average amount the data vary from the mean. The formula for variance (i.e., square of standard deviation $\sigma$) is:

$$ \sigma^2 = \frac{\sum_{i=1}^{n}(x_i - \overline{x})^2} {n-1} $$

$$ \sigma^2 = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(x_i - \overline{x})} {n-1} $$

When two variables are related, changes in one variable are met with similar changes in the other variable. Thus, when one variable deviates from its mean, the other variable should deviate in a similar way.

To do this in R manually, let's consider the hemophil data set. 

```{r}
tempHemophil <- hemophil %>%
  mutate (sdev_activity = Activity - mean(Activity), sdev_Antigen = Antigen - mean(Antigen))
head(tempHemophil)

```

The two variables `Activity` and `Antigen` clearly are related (see scatterplot). They covary because as one variables deviates from the mean in one direction, the other variable deviates from the mean in the same direction.

When there is one variable, we square the deviations to get variance:

$$ \sigma^2 = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(x_i - \overline{x})} {n-1} $$

When there are two variables, we mutliply the deviation for one variable by the corresponding variable for the second variable to get the cross-product deviations:

$$ (x_i - \overline{x})(y_i - \overline{y}) $$

then we sum the cross-product deviations,

$$ \sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y}) $$

and finally we average the sum of all cross-product deviations to get the covariance cov(x, y):

$$ cov(x, y) = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y})} {n-1} $$

We can calculate the covariance in R as:
```{r}
tempHemophil <- tempHemophil %>%
  mutate(crossProduct = sdev_activity * sdev_Antigen, 
         covariance = sum(crossProduct) / (nrow(tempHemophil) - 1)) 
```

Display the covariance

```{r}
head(tempHemophil)
```

The covariance between these 2 variables `Activity` and `Antigen` is 0.01219725

Now, compare this with running `cov` from the stats package:

```{r}
cov(hemophil$Activity, hemophil$Antigen)
```

Now, recall that we are interested in this Covariance topic because we are interested in learning about MANOVA. The concept of variance-covariance matrix comes into picture. 

In a variance-covariance matrix, the diagnol elements represent the `variance` and the non-diagnol elements represent the covariance between the variables.

A positive covariance indicates that as one variable deviates from the mean, the other variables deviates in the same direction. A negative covariance indicates that as one variable deviates from the mean (e.g., increases), the other variable deviates in the opposite direction (e.g., decreases).

However, the size of the covariance depends on the scale of measurement. Larger scale units will lead to larger covariance. To overcome the problem of dependence on measurement scale, we need to convert convariance to a standard set of units through standardisation by dividing the covariance by the standard deviation (i.e., similar to how we compute z-scores).

With two variables, here are two standard deviations. We simply multiply the two standard deviations $ \sigma_{x}*\sigma_{y} $ We divide the covariance by the product of the two standard deviations to get the standardised covariance, which is know n as a correlation coefficient r:

$$ r = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y})} {(n-1)(\sigma_{x}\sigma_{y})} $$

```{r}
(pearsonR <- cov(hemophil$Activity, hemophil$Antigen) / (sd(hemophil$Activity) * sd(hemophil$Antigen)))
```

To calculate the p-value:

```{r}
cor.test(hemophil$Activity, hemophil$Antigen)$p.value
```

> Note: The value of the covariance depends on the scale of measurement. A larger scale will yield a large covariance. To overcome the problem of dependence on measurement scale, we need to convert convariance to a standard set of units through standardisation by dividing the covariance by the standard deviation. By dividing the covariance of two variables by the product of their sd, we will get the correlation coefficient. 

Coming back to the main question:

Provide a scatterplot for Carrier vs Non-Carrier for your final variables used in #2 and color code the points by group. Since we know each variable satistifies normality from #2, the only additional question is if the data are following multivariate normality with a constant covariance matrix for each group. Try to explain what visual properties you see in the plot would lead you for or against the assumption of constant covariance matrix. Recall covariance matrix is simply book keeping variance and correlation estimates for the two variables. 

```{r}
# Scatter plot of Carrier vs Non-Carrier
ggplot(data = hemophil, aes(x = Activity, y = Antigen)) +
  geom_point() +
  facet_grid(.~ factor(Group))
```

